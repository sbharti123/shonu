{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classfier Selection for Message Count Vector",
    "# SOEN7481 class project\n",
    "# @author: sbharti\n",
    "# @version: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ClassfiersFinder\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.text('/home/sumit/HDFS.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer,StringIndexer \n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, PCA\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "from pyspark.sql.types import StringType, LongType, IntegerType\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "\n",
    "import calendar, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The goal of this file is to select the unique unique classfiers to be used for Message Count Vector.\n",
    "# The Log data is in a text file which requires transformation for creating the Message Count Vector.\n",
    "# The classifiers are the verbs ending in 'ing' and 'ed' \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution times are based on a 6thGen i7-6700K CPU at 4Ghz with 8 logical cores (4 physical cores with HyperThreading)\n",
    "# Additional HW properties: 16GB RAM (DDR4 at 3000 MHz) and SSD performance at 866 MB/s\n",
    "# PySpark and dependencies run on an Ubuntu 18 based host within Windows Subsystem for Linux on Windows 10 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define HostFinder type overriding Transformer type for Pipeline chaining\n",
    "class HostFinder(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(HostFinder, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def findHost(array_variable):\n",
    "            for i in array_variable:\n",
    "                if (isinstance(i, str)):\n",
    "                    if (\"10.25\" in i):\n",
    "                        for j, c in enumerate(i):\n",
    "                            if c.isdigit():\n",
    "                                if (i.find(\":\")==-1):\n",
    "                                    # columns cannot have \".\" in their names\n",
    "                                    tmp = i[j:]\n",
    "                                    tmp = tmp.replace(\".\", \"_\")\n",
    "                                    return tmp\n",
    "                                else:\n",
    "                                    tmp = i[j:i.find(\":\")]\n",
    "                                    # columns cannot have \".\" in their names\n",
    "                                    tmp = tmp.replace(\".\", \"_\")\n",
    "                                    return tmp\n",
    "            return None\n",
    "        \n",
    "        outputCol = self.getOutputCol()\n",
    "        #in_col = dataset[self.getInputCol()]\n",
    "        inputCol = self.getInputCol()\n",
    "        \n",
    "        findHostUDF = f.udf(lambda z: findHost(z), StringType())\n",
    "        \n",
    "        dataset = dataset.withColumn(outputCol, findHostUDF(f.col(inputCol)))\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define TimeStampBuilder type overriding Transformer type for Pipeline chaining\n",
    "class TimeStampBuilder(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(TimeStampBuilder, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def buildTimeStamps(an_array):\n",
    "            return an_array[0] + \" \" + an_array[1]\n",
    "\n",
    "        outputCol = self.getOutputCol()\n",
    "        inputCol = self.getInputCol()\n",
    "\n",
    "        buildTimeStampsUDF = f.udf(lambda z: buildTimeStamps(z), StringType())\n",
    "\n",
    "        dataset = dataset.withColumn(outputCol, buildTimeStampsUDF(f.col(inputCol)))\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define EpochBuilder type overriding Transformer type for Pipeline chaining\n",
    "class EpochBuilder(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(EpochBuilder, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        def buildEpoch(tsValue): \n",
    "            return calendar.timegm(time.strptime(tsValue, \"%y%m%d %H%M%S\"))\n",
    "\n",
    "        outputCol = self.getOutputCol()\n",
    "        inputCol = self.getInputCol()\n",
    "\n",
    "        buildEpochUDF = f.udf(lambda z: buildEpoch(z), LongType())\n",
    "        \n",
    "        dataset = dataset.withColumn(outputCol, buildEpochUDF(f.col(inputCol)))\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define WindowNumberBuilder type (for the time series) overriding Transformer type for Pipeline chaining\n",
    "class WindowNumberBuilder(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(WindowNumberBuilder, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        # creating windows must be based on the reference since multiple threads \n",
    "        # will split the dataset and work on chunks\n",
    "        # and they all must have the same starting point\n",
    "        windowBaseEpoch = dataset.first().asDict()[\"epochValue\"]\n",
    "\n",
    "        def calculateWindowNumber(epochValue, stepSize):\n",
    "            #global currentWindowBaseEpoch\n",
    "            # get only the floor part of the division\n",
    "            windowNumber = (epochValue - windowBaseEpoch) // stepSize\n",
    "            return windowNumber\n",
    "                \n",
    "        outputCol = self.getOutputCol()\n",
    "        inputCol = self.getInputCol()\n",
    "\n",
    "        calculateWindowNumberUDF = f.udf(lambda z: calculateWindowNumber(z, 5), IntegerType())\n",
    "        \n",
    "        dataset = dataset.withColumn(outputCol, calculateWindowNumberUDF(f.col(inputCol)))\n",
    "        return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline stages\n",
    "tokenizer = RegexTokenizer(inputCol=\"value\", outputCol=\"token_text\", pattern=":\s+)\n",
    "stopremove = StopWordsRemover(inputCol=\"token_text\", outputCol=\"no_stop_tokens\")\n",
    "hashingTF = HashingTF(inputCol=\"no_stop_tokens\", outputCol=\"rawFeatures\", numFeatures=29)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"tf_IDF\")\n",
    "clean_up = VectorAssembler(inputCols=['tf_IDF'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[tokenizer,stopremove,hashingTF,idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11175629"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['value', 'token_text', 'no_stop_tokens', 'rawFeatures', 'tf_IDF', 'features']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_columns = clean_data.withColumn('value', f.split(clean_data['value'],'\\\\,'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_tokens = clean_data.select('token_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "only_tokens.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findVerbs(array_variable):\n",
    "    for i in array_variable:\n",
    "        if (isinstance(i, str)):\n",
    "            if (i.endswith('ing') or i.endswith('ed')):\n",
    "                return i\n",
    "    return 'not_found'"
    ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "findVerbsUDF = f.udf (lambda z: findVerbs(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_verbs=only_tokens.withColumn(\"verbs\", findVerbsUDF(f.col(\"token_text\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token_text: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- verbs: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_with_verbs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = text_with_verbs.groupBy(\"verbs\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- verbs: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+\n",
      "|summary|            verbs                 count|\n",
      "+-------+--------------------+------------------+\n",
      "|  count|              580406|            580406|\n",
      "|   mean|                null|19.254847468840776|\n",
      "| stddev|                null| 5.439656641923291|\n",
      "|    min|blk_-100000252996...|                 1|\n",
      "|    25%|                null|                19|\n",
      "|    50%|                null|                19|\n",
      "|    75%|                null|                20|\n",
      "|    max|blk_9999802597045...|               298|\n",
      "+-------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counted.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580406\n"
     ]
    }
   ],
   "source": [
    "print (counted.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
    }
   ],
   "source": [
    "counted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
